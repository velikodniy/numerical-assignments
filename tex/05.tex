\section{Поиск экстремумов функций}

\Aim{Исследовать методы поиска экстремумов функций одной и нескольких
  переменных при помощи электронных вычислительных машин.}

\subsection{Основные понятия}

Экстремальные задачи — это задачи на нахождение экстремума (минимума
или максимума) некоторой функции. Процесс поиска экстремума называется
оптимизацией.
\begin{defn}
Функция $f(x)$ имеет локальный минимум (максимум) в точке $x^{*},$
если существует такое $\delta$, что для любого $x$ из $\delta$-окрестности
$|x-x^{*}|<\delta$ выполняется равенство 
\[
f(x^{*})\leqslant f(x)\;(f(x^{*})\geqslant f(x)).
\]

\end{defn}

\begin{defn}
Функция $f(x),$ заданная на множестве $U$ имеет глобальный минимум
(максимум) в точке $x^{*}$, если $f(x^{*})=\inf_{U}f(x)$ ($f(x^{*})=\sup_{U}f(x)$).
\end{defn}
Без потери общности можно рассматривать только поиск минимума функции,
так как задача поиска максимума некоторой функции $f(x)$ является
одновременно задачей поиска минимума функции $-f(x).$

Часто также рассматривают задачи поиска условного экстремума, когда
на решение накладываются дополнительные ограничения. Частным случаем
задач поиска условного экстремума являются задачи математического
программирования.

Методы оптимизации часто используются как вспомогательные в других
методах. Например, при решении задачи регрессии или машинном обучении. 

Пусть оптимизируемая функция $f(x)$ определена на $\mathbb{R}$.
\begin{defn}
Функция называется унимодальной на некотором отрезке, если существует
точка, делящая его на две части, в одной из которых функция убывает,
а в другой — возрастает.
\end{defn}
Для корректной работы численных методов необходимо, чтобы на рассматриваемом
отрезке функция была унимодальной. Для этого может потребоваться провести
предварительное исследование.


\begin{defn}
Точка $x^{*}$ называется стационарной точкой, если
\[
f\text{'}(x^{*})=0.
\]
\end{defn}
\begin{thm}
Для того, чтобы стационарная точка $x^{*}$ была точкой локального
минимума дважды дифференцируемой функции $f(x)$ достаточно, чтобы
выполнялось неравенство
\[
f''(x^{*})>0.
\]

\end{thm}
Определение и теорему можно обобщить и на случай функции от нескольких
переменных.
\begin{defn}
Точка $\mathbf{x}=(x_{1},x_{2},\dots,x_{n})$ называется стационарной,
если для всех $i$ выполняется 
\[
\frac{\partial}{\partial x_{i}}f(x_{1},x_{2},\dots,x_{n})=0.
\]
\end{defn}
\begin{thm}
Для того, чтобы чтобы стационарная точка $\mathbf{x}=(x_{1},x_{2},\dots,x_{n})$
была точкой локального минимума дважды дифференцируемой функции $f(x_{1},x_{2},\dots,x_{n})$
достаточно, чтобы в ней матрица Гессе 
\[
\begin{pmatrix}\frac{\partial^{2}f}{\partial x_{1}^{2}} & \frac{\partial^{2}f}{\partial x_{1}\partial x_{2}} & \ldots & \frac{\partial^{2}f}{\partial x_{1}\partial x_{n}}\\
\frac{\partial^{2}f}{\partial x_{2}\partial x_{1}} & \frac{\partial^{2}f}{\partial x_{2}^{2}} & \ldots & \frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial^{2}f}{\partial x_{n}\partial x_{1}} & \frac{\partial^{2}f}{\partial x_{n}\partial x_{2}} & \ldots & \frac{\partial^{2}f}{\partial x_{n}^{2}}
\end{pmatrix}
\]
была положительно определена (т.е., например, все её собственные значения
были положительными).
\end{thm}

\subsection{Оптимизация функции одной переменной}


\paragraph{Метод деления пополам (дихотомии)}

Идея метода заключается в последовательном делении отрезка, на котором
функция унимодальна, на две части. Причём на каждом шаге выбирается
часть, содержащая локальный экстремум.

Допустим, в качестве исходного взят отрезок $[a,b]$. 

Чтобы определить, в каком из подотрезков находится экстремум, требуется
вычислить значения функции в двух пробных точках, расположенных по
обе стороны от его центра:
\[
c^{-}=\frac{a+b-\Delta}{2},\; c^{+}=\frac{a+b+\Delta}{2},
\]
где $\Delta$ — достаточно малое число. Т.к. из-за малости $\Delta$
пробные точки близки к середине, отрезок будет каждый раз делиться
приблизительно в два раза. Тогда количество шагов, необходимых для
достижения погрешности $\varepsilon$ можно оценить по формуле
\[
N\approx\log_{2}\frac{b-a}{\varepsilon}.
\]


Если $f(c^{-})<f(c^{+})$, то локальный минимум находится в левом
из перекрывающихся подотрезков $[a,c^{+}]$. В противном случае —
в правом $[c^{-},b]$. Раенство значений указывает на то, что середина
отрезка является окрестностью локального минимума.

При достижении необходимой точности в качестве приближённого значения
минимума берётся центр соответствующего подотрезка.


\paragraph{Метод золотого сечения}

Пробные точки можно расположить таким образом, чтобы одна из них стала
пробной на новом отрезке. Это позволит сократить количество вычислений
в случае сложной функции $f(x).$

Для этого отрезок нужно делить в пропорции золотого сечения, равного
$\tau=\frac{\sqrt{5}-1}{2}\approx0{,}61803.$ (\emph{Доказать.}) В
случае отрезка $[0,1]$ пробными точками будут $c^{-}=1-\tau$ и $c^{+}=\tau$.

Погрешность вычислений на шаге $N$ будет равна
\[
\varepsilon_{N}=(b-a)\tau^{N}.
\]
Отсюда получаем количество шагов, необходимое для достижения точности
$\varepsilon$
\[
N=\frac{\ln\varepsilon-\ln(b-a)}{\ln\tau}.
\]



\paragraph{Метод Ньютона}

Если функция унимодальна на некотором отрезке, то можно использовать
метод Ньютона, для решения уравнения $f'(x)=0.$ Это позволит найти
точку, подозрительную на экстремум.


\subsection{Интерполяционные методы}

В рассмотренных ранее методах, значения функции исследовались лишь
в нескольких точках. Идея интерполяционных методов заключается в исследовании
сразу во многих точках. Для этого исходная функция $f(x)$ аппроксимируется
полиномом, а его минимум служит прибижением к искомому минимуму. Как
правило пользуются полиномами второй и третьей степени, которые обеспечивают
достаточную точность при сравнительно низких вычислительных затратах.
Кроме того, для полиномов высоких степеней существует проблема появления
нескольких экстремумов.

Рассмотрим параболическую интерполяцию. Для этого на отрезке выбираем
три точки: $x_{1},x_{2},x_{3}.$

Пусть $f(x_{1})=f_{1}$, $f(x_{2})=f_{2}$, $f(x_{3})=f_{3}$.

Методом Ньютона можно найти коэффициенты интерполяционного полинома
\[
P_{2}(x)=a_{0}+a_{1}(x-x_{1})+a_{2}(x-x_{1})(x-x_{2}),
\]
где
\begin{eqnarray*}
a_{0} & = & f_{1},\\
a_{1} & = & \frac{f_{2}-f_{1}}{x_{2}-x_{1}},\\
a_{2} & = & \frac{1}{x_{3}-x_{1}}\left(\frac{f_{3}-f_{2}}{x_{3}-x_{2}}-\frac{f_{2}-f_{1}}{x_{2}-x_{1}}\right).
\end{eqnarray*}


Точку минимума полинома можно найти, приравняв первую производную
к нулю:
\[
\bar{x}=\frac{1}{2}(x_{1}+x_{2}-\frac{a_{1}}{a_{2}}).
\]


Эта процедура повторяется до достижения необходимой точности. Следующие
три точки можно выбирать, например, сокращая исходный отрезок по аналогии
с расмотренными ранее методами.


\subsection{Оптимизация функций нескольких переменных}


\paragraph{Метод покоординатного спуска}

Это один из самых простых методов поиска минимума функции от нескольких
переменных. Его суть состоит в последовательном поиске минимума по
каждой из координат при фиксированных остальных. Для минимизации можно
использовать любой из рассмотренных ранее одномерных методов.

Если оптимизируется функция $f(x_{1},x_{2},\dots,x_{n})$, то за одну
итерацию метода потребуется выполнить $n$ одномерных оптимизаций.

Критерием прекращения итераций может служить, например, близость результатов
двух последовательных шагов.

\emph{Картинки с линиями уровня}

Можно показать, что метод покоординатного спуска сходится к точке
локального минимума, если существуют вторые производные функции $f$
и при этом существут такие константы $c_{1},c_{2},c_{3}>0,$ что
\[
f''_{x_{1}}\geqslant c_{1},\; f''_{x_{2}}\geqslant c_{2},\; f''_{x_{1}x_{2}}\geqslant c_{3}
\]
и при этом
\[
c_{1}c_{2}>c_{3}^{2}.
\]


Метод сходится достаточно медленно, особенно при наличии так называемых
«оврагов».


\paragraph{Метод градиентного спуска}

Градиент функции $f$ — это вектор
\[
\mathrm{grad}f=\nabla f=\left(\frac{\partial f}{\partial x_{1}},\frac{\partial f}{\partial x_{2}},\dots,\frac{\partial f}{\partial x_{n}}\right).
\]


Особенностью градиента является то, что он всегда направлен в сторону
наибольшего роста функции. Очевидно, вектор $(-\nabla f)$ направлен
в сторону локального минимума. В самой точке минимума (как, впрочем,
и в седловых точках) $\nabla f=0$.

Таким образом, можно построить процесс последовательного приближения
к точке минимума от некоторого начального вектора $\mathbf{x}^{0}$:
\[
\mathbf{x}^{n+1}=\mathbf{x}^{n}-\eta\nabla f(\mathbf{x}^{n}).
\]
Здесь $\eta$ — параметр метода, задающий величину шага на каждой
итерации. Если он будет слишком мал, то для поиска минимума потребуется
лишком большое число итераций. Если слишком велик — есть риск «проскочить»
точку экстремума.

Итерации продолжаются до получения заданной точности. В качестве условия
можно взять, например, следующее:
\[
\|\nabla f(\mathbf{x}^{n+1})\|\leqslant\varepsilon.
\]


Вообще говоря, шаг метода градиентного спуска может быть переменным
и меняться ка каждой итерации. Поэтому желательно его выбрать так,
чтобы функция максималльно уменьшала своё значение. Для выбора такого
шага может потребоваться решить одномерную задачу оптимизации относительно
$\eta$:
\[
\eta=\arg\min_{\eta}f(\mathbf{x}^{n}-\eta\nabla f(\mathbf{x}^{n})).
\]


В этом случае говорят о методе наискорейшего спуска.



Задача:

Необходимо запрограммировать и исследовать следующие методы:

метод дихотомии; метод покоординатного спуска; метод градиентного
спуска.

При желании можно также реализовать:

метод золотого сечения.

В результатах должно быть представлено:

исходные задачи – по одной функции одной и двух переменных, области
унимодальности; точные решения; решения задач, полученные каждым их
методов; задаваемые погрешности для критерия прекращения расчётов;
количество итераций, потребовавшихся для достижения экстремума.

В заключительной части нужно сделать выводы, дающие ответ на следующие
вопросы:

Какой из методов наиболее точный? Какой метод самый быстрый? Каковы
преимущества каждого из методов?

